{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "import json\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "from dask.distributed import Client\n",
    "import fsspec\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.ops import cascaded_union\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "\n",
    "## And a bunch of carbonplan dependencies\n",
    "from carbonplan_data import cat as core_cat\n",
    "\n",
    "from carbonplan_forest_offsets.utils import aa_code_to_ss_code\n",
    "from carbonplan_forest_offsets.data import cat, get_temp_bucket\n",
    "from carbonplan_forest_offsets.analysis.assign_project_fldtypcd import load_classification_data\n",
    "from carbonplan_forest_offsets.load.geometry import (\n",
    "    get_overlapping_states,\n",
    "    load_supersections,\n",
    ")\n",
    "from carbonplan_forest_offsets.load.project_db import load_project_db\n",
    "from carbonplan_forest_offsets.load.geometry import load_supersections\n",
    "\n",
    "\n",
    "plt.rcParams.update({\"font.size\": 14, \"svg.fonttype\": \"none\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aoi(ss_ids):\n",
    "    da = core_cat.nlcd.raster(region=\"conus\").to_dask()\n",
    "    crs = da.attrs[\"crs\"]\n",
    "\n",
    "    supersections = load_supersections().to_crs(crs)\n",
    "\n",
    "    subset_supersection = supersections[supersections[\"ss_id\"].isin(ss_ids)].copy()\n",
    "    subset_supersection.loc[:, \"dissolve_all\"] = 1\n",
    "\n",
    "    aoi = subset_supersection.dissolve(by=\"dissolve_all\").buffer(150_000).to_crs(\"epsg:4326\").item()\n",
    "    return aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def species_array_to_d(species_array):\n",
    "    return {str(species[\"code\"]): round(species[\"fraction\"], 4) for species in species_array}\n",
    "\n",
    "\n",
    "def load_data(ss_ids):\n",
    "    \"\"\"returns trained classifier and data vectorizer to apply to multiple opr_ids\"\"\"\n",
    "\n",
    "    if (len(ss_ids) == 1) & (ss_ids[0] > 200):\n",
    "        data = load_classification_data([\"ak\"])\n",
    "\n",
    "    else:\n",
    "        da = core_cat.nlcd.raster(region=\"conus\").to_dask()\n",
    "        crs = da.attrs[\"crs\"]\n",
    "\n",
    "        supersections = load_supersections().to_crs(crs)\n",
    "\n",
    "        subset_supersection = supersections[supersections[\"ss_id\"].isin(ss_ids)].copy()\n",
    "        subset_supersection.loc[:, \"dissolve_all\"] = 1\n",
    "\n",
    "        aoi = (\n",
    "            subset_supersection.dissolve(by=\"dissolve_all\")\n",
    "            .buffer(150_000)\n",
    "            .to_crs(\"epsg:4326\")\n",
    "            .item()\n",
    "        )\n",
    "\n",
    "        postal_codes = get_overlapping_states(aoi)\n",
    "        print(f\"preparing to load: {[x for x in postal_codes]}\")\n",
    "        data = load_classification_data(postal_codes, aoi=aoi)\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_regional_classifier(data):\n",
    "    \"\"\"returns trained classifier and data vectorizer to apply to multiple opr_ids\"\"\"\n",
    "\n",
    "    base_clf = RadiusNeighborsClassifier(weights=\"distance\", algorithm=\"brute\", outlier_label=-999)\n",
    "    param_grid = [\n",
    "        {\"radius\": np.arange(0.15, 0.651, 0.025)}\n",
    "    ]  # initial testing never yielded a case where we went above 0.5\n",
    "\n",
    "    print(f\"doing GridSearch \")\n",
    "\n",
    "    clf = GridSearchCV(base_clf, param_grid, cv=5, refit=True, verbose=10)\n",
    "    start = time()\n",
    "    with joblib.parallel_backend(\"dask\"):\n",
    "        clf.fit(data[\"features\"], data[\"targets\"])\n",
    "    print(\"search took %.2f seconds\" % (time() - start))\n",
    "    return clf, data[\"dictvectorizer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_prefix, fs_kwargs = get_temp_bucket()\n",
    "fn = f\"{fs_prefix}/radius_neighbor_params.json\"\n",
    "with fsspec.open(fn, mode=\"r\", **fs_kwargs) as f:\n",
    "    radii = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ss_id, radius in radii.items():\n",
    "    if ss_id not in store:\n",
    "        print(f\"scoring {ss_id}...\")\n",
    "\n",
    "        d = load_data([int(ss_id)])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            d[\"features\"], d[\"targets\"], stratify=d[\"targets\"], test_size=0.2\n",
    "        )\n",
    "        clf = RadiusNeighborsClassifier(\n",
    "            weights=\"distance\", algorithm=\"brute\", outlier_label=-999, radius=radius\n",
    "        )\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        preds = clf.predict(X_test)\n",
    "        scores = (\n",
    "            f1_score(y_test, preds, average=\"weighted\"),\n",
    "            f1_score(y_test, preds, average=\"micro\"),\n",
    "            f1_score(y_test, preds, average=\"macro\"),\n",
    "        )\n",
    "\n",
    "        store[ss_id] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(radii) == len(store):\n",
    "\n",
    "    ss_names = load_supersections(include_ak=False).set_index(\"ss_id\")[\"SSection\"].to_dict()\n",
    "\n",
    "    ss_names = {str(k): v for k, v in ss_names.items()}\n",
    "\n",
    "    renamed_store = {ss_names.get(k, k): v for k, v in store.items()}\n",
    "    del renamed_store[\"285\"]\n",
    "    del renamed_store[\"286\"]\n",
    "    renamed_store[\"Southeast and South Central Alaska\"] = renamed_store[\"287\"]\n",
    "    del renamed_store[\"287\"]\n",
    "\n",
    "    fs_prefix, fs_kwargs = get_retro_bucket()\n",
    "    fn = f\"{fs_prefix}/reclassification/classifier_fscores.json\"\n",
    "\n",
    "    with fsspec.open(fn, mode=\"w\", **fs_kwargs) as f:\n",
    "        json.dump(renamed_store, f, indent=2)\n",
    "else:\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
