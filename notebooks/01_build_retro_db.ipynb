{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"50\" src=\"https://carbonplan-assets.s3.amazonaws.com/monogram/dark-small.png\" style=\"margin-left:0px;margin-top:20px\"/>\n",
    "\n",
    "# Build Retro-DB and Retro-DB-light databases\n",
    "\n",
    "_by Grayson Badgley and Joe Hamman (CarbonPlan), December 17, 2020_\n",
    "\n",
    "This notebook munges a collection of datasets to create what we refer to as the `Retro-DB` and\n",
    "`Retro-DB-light` databases.\n",
    "\n",
    "The source datasets include:\n",
    "\n",
    "- Project-DB: A Google sheet including the digitized forest carbon offset project details.\n",
    "- ARB Issuance table: A spreadsheet including the official ARBOC issuances (version dated September\n",
    "  9, 2020).\n",
    "- MTBS Fire Risk: GeoJSON datasets including aggregated by supersection and ecoregion.\n",
    "- Project shapes: A collection of GeoJSON datasets describing the boundaries of each project in\n",
    "  Project-DB.\n",
    "\n",
    "In addition to the static datasets above, we derive the following datasets:\n",
    "\n",
    "- OPDR-Calculated: issuance derived from IFM-1, IFM-3, IFM-7, IFM-8, and secondary effects (SE).\n",
    "\n",
    "Like all munging workflows, not everything here is perfectly polished, but the resulting datasets\n",
    "are clean and should be useful for holistic analysis of the programs included.\n",
    "\n",
    "At the end of this notebook, we end up with three \"allocation\" values:\n",
    "\n",
    "- Issuance: the official, issued allocation of ARBOCs as recorded by ARB.\n",
    "- OPDR-Reported: the OPO/APD reported ARBC\n",
    "- OPDR-Calculated: issuance derived from IFM-1, IFM-3, IFM-7, IFM-8, and secondary effects (SE).\n",
    "\n",
    "In a separate notebook, we explore cases where these values diverge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "from itertools import permutations\n",
    "import geopandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from carbonplan_retro.load.project_db import load_project_db\n",
    "from carbonplan_retro.load.issuance import load_issuance_table\n",
    "from carbonplan_retro.analysis import allocation\n",
    "\n",
    "# options\n",
    "EXCLUDE_GRADUATED_PROJECTS = True\n",
    "WRITE_SHAPES = False\n",
    "\n",
    "# paths\n",
    "DROPBOX = '/Users/jhamman/CarbonPlan Dropbox/Projects/Microsoft/Forests-Retrospective'\n",
    "BUCKET = f'{DROPBOX}/carbonplan-retro'\n",
    "VERSION = 'v1.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load retro-db and issuance table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_db = load_project_db(\"Forest-Offset-Projects-v0.3\", use_cache=False, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is this strange sub-class of projects within the compliance market which we refer to as\n",
    "\"graduated\" projects -- these are projects that started out in the \"Early Action\" period and later\n",
    "\"graduated\" into the full-fledged compliance program. Unfortunately, these projects tend to be\n",
    "materially deficient and tough to work with -- there are all sorts of issues with getting their\n",
    "numbers correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXCLUDE_GRADUATED_PROJECTS:\n",
    "    project_db = project_db[~project_db[\"project\"][\"early_action\"].str.startswith(\"CAR\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also downloaded the spatial boundaries for each project. Here we simply standardize the\n",
    "GeoJSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if WRITE_SHAPES:\n",
    "    for i, proj in enumerate(project_db.index):\n",
    "        src = f\"{DROPBOX}/shapes/{proj}.json\"\n",
    "        dst = f\"{BUCKET}/projects/{proj}\"\n",
    "        dst_file = os.path.join(dst, \"shape.json\")\n",
    "        os.makedirs(dst, exist_ok=True)\n",
    "\n",
    "        with open(src) as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if len(data[\"features\"]) == 1:\n",
    "            data[\"features\"][0][\"properties\"] = {\"id\": proj}\n",
    "        else:\n",
    "            gdf = geopandas.GeoDataFrame.from_file(src)\n",
    "            data = json.loads(geopandas.GeoDataFrame(geometry=[gdf.unary_union]).to_json())\n",
    "            data[\"features\"][0][\"properties\"] = {\"id\": proj}\n",
    "\n",
    "        with open(dst_file, \"w\") as f:\n",
    "            json.dump(data, f, indent=2, allow_nan=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Issuance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: store this issuance file somewhere else!\n",
    "issuance_table = load_issuance_table(\n",
    "    f\"{DROPBOX}/documents-of-interest/arb/issuance/arboc_issuance_2020-09-09.xlsx\"\n",
    ")\n",
    "\n",
    "# One project has multiple issuance events in its first reporting period, aggregate them\n",
    "agg_by_rp = issuance_table.groupby([\"opr_id\", \"arb_rp_id\"])[[\"allocation\"]].sum()\n",
    "issuance_first_rp = agg_by_rp.xs(\"A\", level=1)[\"allocation\"]\n",
    "issuance_first_rp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPDR-calculated\n",
    "\n",
    "OPDRs report five individual components that we use to recalculate ARBOC issuance:\n",
    "\n",
    "- IFM-1: standing live\n",
    "- IFM-3: standing dead\n",
    "- IFM-7: in-use wood products\n",
    "- IFM-8: landfilled wood products\n",
    "- Secondary Effects: market leakage &etc.\n",
    "\n",
    "We use these five \"components\", as reported in the OPDR for both the Baseline scenario\n",
    "(imaginary/counterfactual) and the Project scenario (what actually happened), to re-derive the ARBOC\n",
    "allocation. This step (i) gives us confidence in the integrity of our data entry and (ii) lays the\n",
    "foundation for _re-calculating_ ARBOCs under different common practice scenarios (see Notebook TK).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opdr_calculated = allocation.calculate_allocation(project_db, round_intermediates=False)\n",
    "\n",
    "opdr_calculated = opdr_calculated\n",
    "opdr_calculated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPDR-Reported\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reported = project_db[(\"rp_1\", \"allocation\", \"\")]\n",
    "reported.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have three Series and can populate the `rp_1.allocation` fields:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_db[(\"rp_1\", \"allocation\", \"reported\")] = reported\n",
    "project_db[(\"rp_1\", \"allocation\", \"calculated\")] = opdr_calculated\n",
    "project_db[(\"rp_1\", \"allocation\", \"issuance\")] = issuance_first_rp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project geometries\n",
    "\n",
    "Here we extract the centroid of each project from the project geometries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add project centroids from shapefiles to a new column\n",
    "coords = [list([]) for i in range(len(project_db))]\n",
    "\n",
    "for i, proj in enumerate(project_db.index):\n",
    "    gdf = geopandas.GeoDataFrame.from_file(f\"{BUCKET}/projects/{proj}/shape.json\")\n",
    "    coords[i] = [gdf.geometry.item().centroid.x, gdf.geometry.item().centroid.y]\n",
    "project_db[(\"project\", \"shape_centroid\", \"\")] = coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire risk\n",
    "\n",
    "In this step, we extract the integrated fire risk for each project based on analysis done\n",
    "aggregating historical fire by supersection and Bailey's ecoregions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use project boundaries and weight by overlap with each polygon.\n",
    "for key in [\"supersections\", \"baileys\"]:\n",
    "    gdf = geopandas.GeoDataFrame.from_file(f\"{BUCKET}/fire/{key}.json\")\n",
    "\n",
    "    fire_risk = []\n",
    "    for i, (proj, row) in enumerate(project_db.iterrows()):\n",
    "        lon, lat = row[(\"project\", \"shape_centroid\", \"\")]\n",
    "        p = Point(lon, lat)\n",
    "        try:\n",
    "            r = -9999\n",
    "            for j in gdf.index:\n",
    "                if gdf.geometry[j].contains(p):\n",
    "                    r = gdf[\"integrated_risk\"][j]\n",
    "                    break\n",
    "            fire_risk.append(r)\n",
    "        except:\n",
    "            fire_risk.append(-9999)\n",
    "\n",
    "    project_db[(\"project\", \"mtbs_fire_risk\", key)] = pd.Series(fire_risk, index=project_db.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Retro-DB light\n",
    "\n",
    "The schema here is istill in flux. Best to just look at the code below for details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rp_1(row):\n",
    "    d = {}\n",
    "    try:\n",
    "        d[\"start_date\"] = row[\"rp_1\", \"start\", \"\"].strftime(\"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        d[\"start_date\"] = None\n",
    "\n",
    "    try:\n",
    "        d[\"end_date\"] = row[\"rp_1\", \"end\", \"\"].strftime(\"%Y-%m-%d\")\n",
    "    except ValueError:\n",
    "        d[\"end_date\"] = None\n",
    "\n",
    "    d.update(row[\"rp_1\"][\"components\"][[\"ifm_1\", \"ifm_3\", \"ifm_7\", \"ifm_8\"]].to_dict())\n",
    "    d[\"secondary_effects\"] = row[\"rp_1\", \"secondary_effects\", \"\"]\n",
    "    d[\"confidence_deduction\"] = row[\"rp_1\", \"confidence_deduction\", \"\"]\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def valid_species(species):\n",
    "\n",
    "    for c in species:\n",
    "        if \"basal_area\" not in c:\n",
    "            print(\"setting missing basal_area to None\")\n",
    "            c[\"basal_area\"] = None\n",
    "        if \"fraction\" not in c:\n",
    "            print(\"setting missing fraction to None\")\n",
    "            c[\"fraction\"] = None\n",
    "    return species\n",
    "\n",
    "\n",
    "def make_supersections(row):\n",
    "    species = row[\"project\", \"species\", \"\"]\n",
    "    if not isinstance(species, dict):\n",
    "        print(\"species was not a dict\")\n",
    "        species = None\n",
    "    else:\n",
    "        species = species.copy()\n",
    "\n",
    "    data = row[\"project\", \"assessment_areas\", \"\"]\n",
    "\n",
    "    if not data:\n",
    "        print(\"assessment_areas was empty or null\")\n",
    "        return []\n",
    "    else:\n",
    "        data = data.copy()\n",
    "\n",
    "    for aa in data:\n",
    "        if species:\n",
    "            if str(aa[\"code\"]) in species:\n",
    "                aa[\"species\"] = valid_species(species[str(aa[\"code\"])])\n",
    "            elif \"all\" in species:\n",
    "                aa[\"species\"] = []\n",
    "            else:\n",
    "                print(\"did not find species key %s\" % aa[\"code\"])\n",
    "                aa[\"species\"] = []\n",
    "\n",
    "        else:\n",
    "            aa[\"species\"] = []\n",
    "    if species and \"all\" in species:\n",
    "        print(\"all field was used\")\n",
    "        data.append({\"code\": 999, \"species\": species[\"all\"]})\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "make_supersections(project_db.loc[\"CAR1094\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_retro_db_light(project_db):\n",
    "\n",
    "    projects = []\n",
    "\n",
    "    for i, row in project_db.iterrows():\n",
    "        print(i)\n",
    "        p = {\n",
    "            \"id\": row[(\"project\", \"opr_id\", \"\")],\n",
    "            \"opr_id\": row[(\"project\", \"opr_id\", \"\")],\n",
    "            \"arb_id\": row[(\"project\", \"arb_id\", \"\")],\n",
    "            \"name\": row[(\"project\", \"name\", \"\")],\n",
    "            \"apd\": row[(\"project\", \"apd\", \"\")],\n",
    "            \"opo\": row[(\"project\", \"opo\", \"\")],\n",
    "            \"owners\": row[(\"project\", \"owners\", \"\")],\n",
    "            \"developers\": row[(\"project\", \"developers\", \"\")],\n",
    "            \"attestor\": row[(\"rp_1\", \"attestation\", \"name\")],\n",
    "            \"is_opo\": row[(\"rp_1\", \"attestation\", \"is_opo\")],\n",
    "            \"coordinates\": row[(\"project\", \"coordinates\", \"\")],\n",
    "            \"shape_centroid\": row[(\"project\", \"shape_centroid\", \"\")],\n",
    "            \"supersection_ids\": row[(\"project\", \"supersection_ids\", \"\")],\n",
    "            \"acreage\": row[(\"project\", \"acreage\", \"\")],\n",
    "            \"buffer_contribution\": row[(\"rp_1\", \"buffer_contribution\", \"\")],\n",
    "            \"arbocs\": {\n",
    "                \"issuance\": row[(\"rp_1\", \"allocation\", \"issuance\")],\n",
    "                \"calculated\": row[(\"rp_1\", \"allocation\", \"calculated\")],\n",
    "                \"reported\": row[(\"rp_1\", \"allocation\", \"reported\")],\n",
    "            },\n",
    "            \"carbon\": {\n",
    "                \"initial_carbon_stock\": {\n",
    "                    \"value\": row[(\"baseline\", \"initial_carbon_stock\", \"\")],\n",
    "                    \"units\": \"tCO2e-1\",\n",
    "                },\n",
    "                \"common_practice\": {\n",
    "                    \"value\": row[(\"baseline\", \"common_practice\", \"\")],\n",
    "                    \"units\": \"tCO2e-1\",\n",
    "                },\n",
    "            },\n",
    "            \"baseline\": row[\"baseline\"][\"components\"][\n",
    "                [\"ifm_1\", \"ifm_3\", \"ifm_7\", \"ifm_8\"]\n",
    "            ].to_dict(),\n",
    "            \"rp_1\": make_rp_1(row),\n",
    "            \"assessment_areas\": make_supersections(row),\n",
    "            \"permanence\": {\n",
    "                \"arb_total_risk\": row[(\"project\", \"reversal_risk\", \"\")],\n",
    "                \"arb_fire_risk\": row[(\"project\", \"fire_risk\", \"\")],\n",
    "                \"mtbs_fire_risk_supersection\": row[(\"project\", \"mtbs_fire_risk\", \"supersections\")],\n",
    "                \"mtbs_fire_risk_baileys\": row[(\"project\", \"mtbs_fire_risk\", \"baileys\")],\n",
    "            },\n",
    "            \"notes\": row[\"project\", \"notes\", \"\"],  # include or not?\n",
    "            \"comment\": \"\",  # get from google sheet\n",
    "        }\n",
    "\n",
    "        for k in [\"coordinates\", \"shape_centroid\"]:\n",
    "            if not p[k]:\n",
    "                p[k] = []\n",
    "            else:\n",
    "                if p[k][0] == -9999:\n",
    "                    p[k] = []\n",
    "\n",
    "        projects.append(p)\n",
    "\n",
    "    return projects\n",
    "\n",
    "\n",
    "retro_db_light = make_retro_db_light(project_db)\n",
    "retro_db_light[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write to JSON:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_projects(project_collection, output):\n",
    "    with open(output, \"w\") as outfile:\n",
    "        json.dump(project_collection, outfile, indent=2)\n",
    "\n",
    "\n",
    "write_projects(retro_db_light, f\"{BUCKET}/projects/retro-db-light-{VERSION}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "write to CSV:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(retro_db_light)\n",
    "df.head()\n",
    "\n",
    "dict_cols = [\"arbocs\", \"carbon\", \"permanence\"]\n",
    "\n",
    "for col_key in dict_cols:\n",
    "    for field_key in df.iloc[0][col_key].keys():\n",
    "        new_key = f\"{col_key}_{field_key}\"\n",
    "        vals = [row[field_key] for row in df[col_key]]\n",
    "        if isinstance(vals[0], dict):\n",
    "            vals = [v[\"value\"] for v in vals]\n",
    "        df[new_key] = vals\n",
    "df = df.drop(columns=dict_cols)\n",
    "\n",
    "df.to_csv(f\"{BUCKET}/projects/retro-db-light-{VERSION}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
