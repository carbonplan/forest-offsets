{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from collections import defaultdict\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from retrospective.load.fia import load_fia_common_practice\n",
    "from retrospective.load.geometry import load_supersections\n",
    "from retrospective.common_practice.common_practice import subset_common_practice_data\n",
    "from retrospective.utils import load_arb_fortypcds\n",
    "from retrospective.utils import aa_code_to_ss_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "sns.set_style(\"white\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_olaf_slag(our_slag, olaf, threshold=75):\n",
    "    comparison = olaf.join(\n",
    "        our_slag.set_index([\"aa_code\", \"site_class\"]),\n",
    "        rsuffix=\"_ours\",\n",
    "        on=[\"aa_code\", \"site_class\"],\n",
    "    )\n",
    "    comparison = comparison.dropna(\n",
    "        subset=[\"slag_co2e_acre_ours\", \"slag_co2e_acre\"]\n",
    "    )  # TODO: has to do w those duplicate aa names\n",
    "\n",
    "    rmse = (\n",
    "        mean_squared_error(comparison[\"slag_co2e_acre_ours\"], comparison[\"slag_co2e_acre\"]) ** 0.5\n",
    "    )\n",
    "    print(f\"SLAG RMSE (all AA): {rmse:.2f}\")\n",
    "\n",
    "    subset = comparison[comparison[\"cond_prop_group\"] > threshold]\n",
    "    subset_rmse = mean_squared_error(subset[\"slag_co2e_acre_ours\"], subset[\"slag_co2e_acre\"]) ** 0.5\n",
    "    print(f\"SLAG RMSE (AA with more thatn {threshold} cond_prop_sum): {subset_rmse:.2f}\")\n",
    "    return comparison\n",
    "\n",
    "\n",
    "def get_bootstrap_bounds(\n",
    "    data,\n",
    "    agg_var=\"slag_co2e_acre\",\n",
    "    statistic=\"mean\",\n",
    "    n_obs=1000,\n",
    "    weights=None,\n",
    "    bootstrap_interval=[0.025, 0.5, 0.975],\n",
    "):\n",
    "    \"\"\"calculate bootstraped confidence intervals for .agg([]) statistic.\n",
    "    Also includes the ability to weight by a column.\n",
    "    For the slag work, i've found that weighting by condprop_unadj helps get closer to ARB's numbers.\n",
    "    \"\"\"\n",
    "\n",
    "    resample = [\n",
    "        data.sample(len(data), weights=weights, replace=True)[agg_var].agg([statistic])[statistic]\n",
    "        for i in range(n_obs)\n",
    "    ]\n",
    "    return pd.Series(resample).quantile(bootstrap_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading up the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conus = [\n",
    "    \"AL\",\n",
    "    \"AZ\",\n",
    "    \"AR\",\n",
    "    \"CA\",\n",
    "    \"CO\",\n",
    "    \"CT\",\n",
    "    \"DE\",\n",
    "    \"FL\",\n",
    "    \"GA\",\n",
    "    \"ID\",\n",
    "    \"IL\",\n",
    "    \"IN\",\n",
    "    \"IA\",\n",
    "    \"KS\",\n",
    "    \"KY\",\n",
    "    \"LA\",\n",
    "    \"ME\",\n",
    "    \"MD\",\n",
    "    \"MA\",\n",
    "    \"MI\",\n",
    "    \"MN\",\n",
    "    \"MS\",\n",
    "    \"MO\",\n",
    "    \"MT\",\n",
    "    \"NE\",\n",
    "    \"NV\",\n",
    "    \"NH\",\n",
    "    \"NJ\",\n",
    "    \"NM\",\n",
    "    \"NY\",\n",
    "    \"NC\",\n",
    "    \"ND\",\n",
    "    \"OH\",\n",
    "    \"OK\",\n",
    "    \"OR\",\n",
    "    \"PA\",\n",
    "    \"RI\",\n",
    "    \"SC\",\n",
    "    \"SD\",\n",
    "    \"TN\",\n",
    "    \"TX\",\n",
    "    \"UT\",\n",
    "    \"VT\",\n",
    "    \"VA\",\n",
    "    \"WA\",\n",
    "    \"WV\",\n",
    "    \"WI\",\n",
    "    \"WY\",\n",
    "]\n",
    "df = load_fia_common_practice(conus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supersections = load_supersections(\"https://storage.googleapis.com/carbonplan-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.sjoin(df, supersections[[\"SSection\", \"geometry\"]], how=\"inner\", op=\"within\")\n",
    "df = df.rename(columns={\"index_right\": \"supersection_id\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some ancillary files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_lut = pd.read_csv(\"/home/jovyan/lost+found/2015_aa_lut.csv\")\n",
    "# this whole analysis excludes alaska -- we drop those assessment areas here so we dont have to wrap this bit in a try/except.\n",
    "arb_lut = arb_lut[arb_lut[\"ss_code\"] != 75]\n",
    "olaf = pd.read_csv(\"/home/jovyan/lost+found/cleaned_olaf.csv\")\n",
    "arb_fortyps = load_arb_fortypcds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective: Getting the right subset of data\n",
    "\n",
    "We've gathered that ARB CP values are based on all FIA plots measured within three windows of time:\n",
    "[2002, 2012], [2006, 2012], or [2008, 2012]. Unfortunately, we do not know the time window on a per\n",
    "supersection basis. So the first step is to try and infer the time window used for calculating\n",
    "Common Practice.\n",
    "\n",
    "We accomplish this by using one of the intermediate output from\n",
    "[Olaf Kuegler](https://ww2.arb.ca.gov/our-work/programs/compliance-offset-program/compliance-offset-protocols/us-forest-projects/2015/common-practice-data),\n",
    "the FIA statistician who ran the queries/analysis the yielded the official ARB CP values. As one of\n",
    "his intermediary analysis steps, he produces `groupings.accdb`, which contains a per assessment sum\n",
    "of the number of proportional conditions (`CONDPROP_UNADJ` in FIAdb) that feed into the later stages\n",
    "of the analysis. This is helpful because we can subset the FIA data in different ways, sum up\n",
    "`CONDPROP_UNADJ`, and compare it against Olaf's number. If we're doing the subsetting correctly, we\n",
    "should get fairly comporable values.\n",
    "\n",
    "Nothing fancy here -- just brute force iteration.\n",
    "\n",
    "We'll store this output in the repository so that we can, if we choose, use the same temporal\n",
    "constraint when exploring CP ourselves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A short aside\n",
    "\n",
    "Part of why this whole thing took me so long to write is I completely re-worked how we subset the\n",
    "underlying FIA data. Still not sure those changes were the best thing in the world, but at least\n",
    "things are a little less brittle. The really _good_ innovation was removing the explicit concept of\n",
    "assessment areas from the subsetting. Eventually, we're goign to want to aggregate by hand-rolled\n",
    "fortypcds, so having everything depending on `aa_code` and `ss_code` just wasn't going to work in\n",
    "the long run.\n",
    "\n",
    "The new approach inovlves explicitly specifying criteria and passing those criteria and the\n",
    "dataframe to a subsetting function. This is going to be pretty nice in the world where we're looking\n",
    "up a handful of projects. But it turns out that it's super slow for doing the bulk iteration we're\n",
    "going to do here. So rather than think carefully about optimizing the queries I just do one join\n",
    "between the supersection dataframe and the condition-slag dataframe. This feels fine because\n",
    "demonstrating that we can honest to god recreate the ARB's CP values, per supersection, is a top\n",
    "priority.\n",
    "\n",
    "We start by building these \"criteria\" dicts -- and then we iterate through those dicts sequentially.\n",
    "Each dict contains four criterion:\n",
    "\n",
    "- supersection_id: this limits the spatial extent of the query.\n",
    "- temporal_filter: min and max MEASYEAR to consider -- we need to figure out which range gets us\n",
    "  closest to ARB.\n",
    "- fortypcds: this gets us to assessment_area/forest community.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varying_year_criteria = []\n",
    "\n",
    "for row in arb_lut.itertuples():\n",
    "\n",
    "    supersection_id = row.ss_code\n",
    "    site_class = row.site_class\n",
    "    fortyps = arb_fortyps[row.aa_code]\n",
    "\n",
    "    for min_year in [2002, 2006, 2008]:\n",
    "        varying_year_criteria.append(\n",
    "            {\n",
    "                \"supersection_id\": supersection_id,\n",
    "                \"temporal_filter\": {\"min_year\": min_year, \"max_year\": 2012},\n",
    "                \"site_class\": site_class,\n",
    "                \"fortyps\": fortyps,\n",
    "                \"aa_code\": row.aa_code,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conds_per_aa = defaultdict(dict)\n",
    "error_supersections = []\n",
    "for criteria in varying_year_criteria:\n",
    "    try:\n",
    "        subset = subset_common_practice_data(\n",
    "            df,\n",
    "            criteria[\"supersection_id\"],\n",
    "            criteria[\"temporal_filter\"],\n",
    "            criteria[\"fortyps\"],\n",
    "            criteria[\"site_class\"],\n",
    "        )\n",
    "        cond_prop_sum = subset.CONDPROP_UNADJ.sum()\n",
    "        olaf_val = olaf[\n",
    "            (olaf[\"aa_code\"] == criteria[\"aa_code\"])\n",
    "            & (olaf[\"site_class\"] == criteria[\"site_class\"])\n",
    "        ][\"cond_prop_group\"].item()\n",
    "        delta_olaf = cond_prop_sum - olaf_val\n",
    "        conds_per_aa[(criteria[\"aa_code\"], criteria[\"site_class\"])][\n",
    "            criteria[\"temporal_filter\"][\"min_year\"]\n",
    "        ] = abs(delta_olaf)\n",
    "    except:\n",
    "        # we have some more work to do with ss_codes 36 and 40 -- there is a whole mess of typos/spelling errors that crop up over and over...forthcoming!\n",
    "        error_supersections.append(criteria[\"supersection_id\"])\n",
    "\n",
    "aa_best_yr = {\n",
    "    k: min(v, key=v.get) for k, v in conds_per_aa.items()\n",
    "}  # select key (year) where `delta_olaf` is smallest -- this means we can no go from aa : min_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    f\"Supersections {[x for x in set(error_supersections)]} did not work. If this message displays anything other than 36 and 40, something has gone wrong\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_criteria = []\n",
    "error_supersections = []\n",
    "\n",
    "for row in arb_lut.itertuples():\n",
    "    try:\n",
    "        supersection_id = row.ss_code\n",
    "        site_class = row.site_class\n",
    "        fortyps = arb_fortyps[row.aa_code]\n",
    "        min_year = aa_best_yr[(row.aa_code, row.site_class)]\n",
    "        optimal_criteria.append(\n",
    "            {\n",
    "                \"supersection_id\": supersection_id,\n",
    "                \"temporal_filter\": {\"min_year\": min_year, \"max_year\": 2012},\n",
    "                \"site_class\": site_class,\n",
    "                \"fortyps\": fortyps,\n",
    "                \"aa_code\": row.aa_code,\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        error_supersections.append(row.ss_code)\n",
    "display(\n",
    "    f\"Supersections {[x for x in set(error_supersections)]} did not work. If this message displays anything other than 36 and 40, something has gone wrong\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more analysis!\n",
    "\n",
    "Alright so we found the right criteria for subsetting the data -- let's subset! First, we start with\n",
    "just naive averaging. It turns out this works pretty well. Note that I am also excluding a handful\n",
    "of conds where CONDPROP_UNDADJ is really tiny. I've found that these examples just cause trouble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unweighted_records = []\n",
    "for criteria in optimal_criteria:\n",
    "    mean_slag = subset_common_practice_data(\n",
    "        df.loc[df[\"CONDPROP_UNADJ\"] > 0.005],\n",
    "        criteria[\"supersection_id\"],\n",
    "        criteria[\"temporal_filter\"],\n",
    "        criteria[\"fortyps\"],\n",
    "        criteria[\"site_class\"],\n",
    "    ).slag_co2e_acre.mean()\n",
    "    unweighted_records.append((criteria[\"aa_code\"], criteria[\"site_class\"], mean_slag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unweighted_slag = pd.DataFrame(\n",
    "    unweighted_records, columns=[\"aa_code\", \"site_class\", \"slag_co2e_acre\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_eight_problem_supersections = [\n",
    "    87,\n",
    "    71,\n",
    "]  # worst offenders are 87, 71. Something goofy going on w parts of 46 & 28\n",
    "xo_aa_codes = [\n",
    "    k for k, v in aa_code_to_ss_code().items() if v in region_eight_problem_supersections\n",
    "]\n",
    "\n",
    "excluded_aa = unweighted_slag.loc[unweighted_slag[\"aa_code\"].isin(xo_aa_codes)]\n",
    "excluded = compare_olaf_slag(excluded_aa, olaf, threshold=30)\n",
    "\n",
    "g = sns.lmplot(\n",
    "    x=\"slag_co2e_acre_ours\",\n",
    "    y=\"slag_co2e_acre\",\n",
    "    data=excluded,\n",
    "    hue=\"ss_code\",\n",
    "    fit_reg=False,\n",
    "    ci=False,\n",
    "    scatter_kws={\"s\": 100, \"alpha\": 0.55},\n",
    ")\n",
    "axes = g.axes.flatten()\n",
    "[ax.set_xlim(0, 300) for ax in axes]\n",
    "[ax.set_ylim(0, 300) for ax in axes]\n",
    "axes[0].set_yticks(np.arange(0, 301, 100))\n",
    "[ax.plot((0, 300), (0, 300), lw=3, ls=\"--\", c=\"r\") for ax in axes]\n",
    "\n",
    "plt.xlabel(\"Recalculated Common Practice\\n(t CO2e acre$^{-1}$)\")\n",
    "plt.ylabel(\"ARB Reported Common Practice\\n(t CO2e acre$^{-1}$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I still can’t explain precisely what is going on, but I can convincingly say that the majority of\n",
    "problems occur in USFS Region 8 [You might recall, Region 8 gave us a bunch of trouble for the\n",
    "mortality work too.] I have two leading theories.\n",
    "\n",
    "- We know that Region 8 has strange sampling patterns — they run out of money and don’t measure\n",
    "  their trees. So it could be the case that when ARB made their numbers, they used a different\n",
    "  subset of data than we’re using. I’m open to this interpretation but after poking around the data\n",
    "  we’re using and the data posted on ARB’s website, I don’t see any crazy differences.\n",
    "- “Access Denied” — Region 8 has lots and lots of plots that don’t get measured because of (i) the\n",
    "  land owner won’t let them (see Texas; cross ref. the Myth of the Rugged Individual) or (ii)\n",
    "  mash/wetland plots that are too dangerous to measure (see Crocodylus acutus). If the ARB’s common\n",
    "  practice calculations attempt to account for missingness — and the missing plots look much\n",
    "  different from sampled plots in terms of carbon stocks — this could drive our disagreements.\n",
    "  Thankfully, we’re not trying to land on the moon and we just need to have a rough idea of what is\n",
    "  going on. So we don’t actually have to discern between the two hypotheses above. Instead, we can\n",
    "  just see what happens when we exclude i) coastal supersections and ii) a big Texas supersection\n",
    "  where disagreeement is rife (and we know “refusals” are high). And lo (see attached), if we\n",
    "  withhold 4 problematic supersections from our analysis, our performance in predicting common\n",
    "  practice increases markedly.\n",
    "\n",
    "N.B. SS 71 and 87 have southern yellow pine (slash/lob &etc) fortyps -- which means they could\n",
    "include some of the productive-ish plantations in the region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_unweighted = unweighted_slag.loc[~unweighted_slag[\"aa_code\"].isin(xo_aa_codes)]\n",
    "subset = compare_olaf_slag(subset_unweighted, olaf)\n",
    "\n",
    "\n",
    "def compare_cp_plot(data):\n",
    "    g = sns.lmplot(\n",
    "        x=\"slag_co2e_acre_ours\",\n",
    "        y=\"slag_co2e_acre\",\n",
    "        data=data,\n",
    "        fit_reg=False,\n",
    "        ci=False,\n",
    "        scatter_kws={\"s\": 100, \"color\": \".3\", \"alpha\": 0.55},\n",
    "    )\n",
    "    axes = g.axes.flatten()\n",
    "    [ax.set_xlim(0, 300) for ax in axes]\n",
    "    [ax.set_ylim(0, 300) for ax in axes]\n",
    "    axes[0].set_yticks(np.arange(0, 301, 100))\n",
    "    [ax.plot((0, 300), (0, 300), lw=3, ls=\"--\", c=\"r\") for ax in axes]\n",
    "    rmse = mean_squared_error(data[\"slag_co2e_acre_ours\"], data[\"slag_co2e_acre\"]) ** 0.5\n",
    "\n",
    "    axes[0].an\n",
    "    plt.xlabel(\"Recalculated Common Practice\\n(t CO2e acre$^{-1}$)\")\n",
    "    plt.ylabel(\"ARB Reported Common Practice\\n(t CO2e acre$^{-1}$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And all together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compare_olaf_slag(unweighted_slag, olaf)\n",
    "\n",
    "g = sns.lmplot(\n",
    "    x=\"slag_co2e_acre_ours\",\n",
    "    y=\"slag_co2e_acre\",\n",
    "    data=res,\n",
    "    fit_reg=False,\n",
    "    ci=False,\n",
    "    scatter_kws={\"s\": 100, \"color\": \".3\", \"alpha\": 0.55},\n",
    ")\n",
    "axes = g.axes.flatten()\n",
    "[ax.set_xlim(0, 300) for ax in axes]\n",
    "[ax.set_ylim(0, 300) for ax in axes]\n",
    "axes[0].set_yticks(np.arange(0, 301, 100))\n",
    "[ax.plot((0, 300), (0, 300), lw=3, ls=\"--\", c=\"r\") for ax in axes]\n",
    "\n",
    "plt.xlabel(\"Recalculated Common Practice\\n(t CO2e acre$^{-1}$)\")\n",
    "plt.ylabel(\"ARB Reported Common Practice\\n(t CO2e acre$^{-1}$)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance from lens of a linear model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smf.ols(\"slag_co2e_acre~slag_co2e_acre_ours\", data=res).fit().params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small intercept and slope is pretty near one. Hurray! RMSE lf 10.11 (or close), which is reduced if\n",
    "we chop off assessment areas with a low count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# with n_obs = 1000, this takes 13 minutes but is embarassingly dask-able if you're impatient. I just got get more coffee\n",
    "\n",
    "\n",
    "weighted_records = {} # this is a little different because i want to keep around the bounds\n",
    "for criteria in optimal_criteria:\n",
    "    try:\n",
    "        subset = subset_common_practice_data(df.loc[df['CONDPROP_UNADJ'] > 0.005], \n",
    "                                    criteria['supersection_id'], \n",
    "                                    criteria['temporal_filter'], \n",
    "                                    criteria['fortyps'],\n",
    "                                    criteria['site_class'])\n",
    "\n",
    "        res = get_bootstrap_bounds(subset, n_obs=250, weights='CONDPROP_UNADJ')\n",
    "        weighted_records[(criteria['aa_code'], criteria['site_class'])] = res\n",
    "    except:\n",
    "        # caused by the ghost superseciton, Ouachita Mixed Forest\n",
    "        print(criteria['aa_code'])\n",
    "\n",
    "#weighted_slag = pd.DataFrame(weighted_records, columns=['aa_code', 'site_class', 'slag_co2e_acre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_bounds = pd.DataFrame(weighted_records).T\n",
    "weighted_slag = (\n",
    "    weighted_bounds[0.5]\n",
    "    .reset_index()\n",
    "    .rename(\n",
    "        columns={\n",
    "            \"level_0\": \"aa_code\",\n",
    "            \"level_1\": \"site_class\",\n",
    "            0.5: \"slag_co2e_acre_ours\",\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_slag = weighted_slag.loc[~weighted_slag[\"aa_code\"].isin(xo_aa_codes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_res = compare_olaf_slag(weighted_slag, olaf, threshold=30)\n",
    "\n",
    "\n",
    "def compare_cp_plot(data):\n",
    "    g = sns.lmplot(\n",
    "        x=\"slag_co2e_acre_ours\",\n",
    "        y=\"slag_co2e_acre\",\n",
    "        data=data,\n",
    "        fit_reg=False,\n",
    "        ci=False,\n",
    "        scatter_kws={\"s\": 100, \"color\": \".3\", \"alpha\": 0.55},\n",
    "    )\n",
    "    axes = g.axes.flatten()\n",
    "    [ax.set_xlim(0, 300) for ax in axes]\n",
    "    [ax.set_ylim(0, 300) for ax in axes]\n",
    "    axes[0].set_yticks(np.arange(0, 301, 100))\n",
    "    [ax.plot((0, 300), (0, 300), lw=3, ls=\"--\", c=\"r\") for ax in axes]\n",
    "    params = smf.ols(\"slag_co2e_acre~slag_co2e_acre_ours\", data=data).fit().params\n",
    "    rmse = mean_squared_error(data[\"slag_co2e_acre_ours\"], data[\"slag_co2e_acre\"]) ** 0.5\n",
    "\n",
    "    axes[0].annotate(\n",
    "        f\"Intercept: {params['Intercept']:.2f}\\nSlope: {params['slag_co2e_acre_ours']:.2f}\\nRMSE: {rmse:.2f}\",\n",
    "        xy=(0.60, 0.25),\n",
    "        xycoords=\"figure fraction\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Recalculated Common Practice\\n(tCO2e acre$^{-1}$)\")\n",
    "    plt.ylabel(\"ARB Reported Common Practice\\n(tCO2e acre$^{-1}$)\")\n",
    "\n",
    "\n",
    "compare_cp_plot(w_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling really smoothes things out. It might not seem like much, but RMSE drops, the intercept gets\n",
    "smaller, slope gets closer to one (in fact its > 1 now, driven by those handful of points where we\n",
    "underestimate common practice).\n",
    "\n",
    "This is great -- it builds confdience that we're capable of recreating CP. I should actually clean\n",
    "this above figure up with some annotations etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_bounds = (\n",
    "    pd.DataFrame(weighted_records)\n",
    "    .T.reset_index()\n",
    "    .rename(columns={\"level_0\": \"aa_code\", \"level_1\": \"site_class\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = olaf.join(\n",
    "    weighted_bounds.set_index([\"aa_code\", \"site_class\"]),\n",
    "    on=[\"aa_code\", \"site_class\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraction_within_ci(data):\n",
    "    inside = data[(data[\"slag_co2e_acre\"] > data[0.025]) & (data[\"slag_co2e_acre\"] < data[0.975])]\n",
    "\n",
    "    outside = data[\n",
    "        (data[\"slag_co2e_acre\"] <= data[0.025]) | (data[\"slag_co2e_acre\"] >= data[0.975])\n",
    "    ]\n",
    "    display(\n",
    "        f\"{len(inside)/len(data)*100:.2f} percent of ARB CP values fall within our bootstrapped 95th confidence intervals of mean CP\"\n",
    "    )\n",
    "    return (inside, outside)\n",
    "\n",
    "\n",
    "inside, outside = fraction_within_ci(full.loc[~full.ss_code.isin([71, 87, 63])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraction_within_ci(data):\n",
    "    inside = data[(data[\"slag_co2e_acre\"] > data[0.025]) & (data[\"slag_co2e_acre\"] < data[0.975])]\n",
    "\n",
    "    outside = data[\n",
    "        (data[\"slag_co2e_acre\"] <= data[0.025]) | (data[\"slag_co2e_acre\"] >= data[0.975])\n",
    "    ]\n",
    "    return len(inside) / len(data)\n",
    "\n",
    "\n",
    "full.groupby(\"ss_code\").apply(fraction_within_ci).sort_values().head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notes on assessment areas falling outside CI\n",
    "\n",
    "Nothign stands out as remarkable for what makes these 70 SS difficult to get...\n",
    "\n",
    "- not something systematic about sample size.\n",
    "- nor is it something systematic about standard deviation/mean...\n",
    "\n",
    "however, on balance, they seem to occur in USFS Region 8/Southern Section of FIA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we don't do the variable year thing?\n",
    "\n",
    "What if we didnt bother to figure out the right time interval? We re-create our criteria filters,\n",
    "but this time we fix `min_year = 2002`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_min_year = []\n",
    "for row in arb_lut.itertuples():\n",
    "    try:\n",
    "        supersection_id = row.ss_code\n",
    "        site_class = row.site_class\n",
    "        fortyps = arb_fortyps[row.aa_code]\n",
    "        min_year = aa_best_yr[(row.aa_code, row.site_class)]\n",
    "        fixed_min_year.append(\n",
    "            {\n",
    "                \"supersection_id\": supersection_id,\n",
    "                \"temporal_filter\": {\"min_year\": 2002, \"max_year\": 2012},\n",
    "                \"site_class\": site_class,\n",
    "                \"fortyps\": fortyps,\n",
    "                \"aa_code\": row.aa_code,\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        pass  # yolo this is just quick example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_year_records = []\n",
    "for criteria in fixed_min_year:\n",
    "    mean_slag = subset_common_practice_data(\n",
    "        df.loc[df[\"CONDPROP_UNADJ\"] > 0.005],\n",
    "        criteria[\"supersection_id\"],\n",
    "        criteria[\"temporal_filter\"],\n",
    "        criteria[\"fortyps\"],\n",
    "        criteria[\"site_class\"],\n",
    "    ).slag_co2e_acre.mean()\n",
    "    fixed_year_records.append((criteria[\"aa_code\"], criteria[\"site_class\"], mean_slag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_slag = weighted_slag = pd.DataFrame(\n",
    "    fixed_year_records, columns=[\"aa_code\", \"site_class\", \"slag_co2e_acre\"]\n",
    ")\n",
    "\n",
    "f_res = compare_olaf_slag(fixed_slag, olaf)\n",
    "\n",
    "g = sns.lmplot(\n",
    "    x=\"slag_co2e_acre_ours\",\n",
    "    y=\"slag_co2e_acre\",\n",
    "    data=f_res,\n",
    "    fit_reg=False,\n",
    "    ci=False,\n",
    "    scatter_kws={\"s\": 100, \"color\": \".3\", \"alpha\": 0.55},\n",
    ")\n",
    "axes = g.axes.flatten()\n",
    "[ax.set_xlim(0, 300) for ax in axes]\n",
    "[ax.set_ylim(0, 300) for ax in axes]\n",
    "axes[0].set_yticks(np.arange(0, 301, 100))\n",
    "[ax.plot((0, 300), (0, 300), lw=3, ls=\"--\", c=\"r\") for ax in axes]\n",
    "\n",
    "plt.xlabel(\"Recalculated Common Practice\\n(t CO2e acre$^{-1}$)\")\n",
    "plt.ylabel(\"ARB Reported Common Practice\\n(t CO2e acre$^{-1}$)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smf.ols(\"slag_co2e_acre~slag_co2e_acre_ours\", data=f_res).fit().params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So yeah it turns out this isnt that big of a deal in aggreate. Intercept changes the most, but slope\n",
    "is quite similar. In effect, this means that all the extra work in this notebook -- and the efforts\n",
    "to make it so we can have variable aggregation through time are largely unneeded. But we didn't know\n",
    "that beforehand!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARB says in the future\n",
    "[they'll include any assessment area with more than 30 FIA plots](https://ww2.arb.ca.gov/sites/default/files/classic//cc/capandtrade/offsets/guidance.document.amended.2015.pdf)\n",
    "(!):\n",
    "\n",
    "> Addition of new assessment areas will follow ARB protocols and will require a minimum of 2 years\n",
    "> of data on a minimum of 30 FIA plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left over from debug TX\n",
    "\n",
    "df[(df['ECOSUBCD'].str.strip().str[:4].isin(['255D', '255B', '255C'])) & (df['BALIVE'] >\n",
    "0)].groupby('FORTYPCD').BALIVE.agg(['mean', 'count']) from retrospective.utils import\n",
    "get_state_boundaries\n",
    "\n",
    "bounds = get_state_boundaries(['tx', 'la', 'al', 'fl'])\n",
    "\n",
    "tx_and_south_ss_codes = gpd.sjoin(bounds, supersections)['index_right'].unique().tolist()\n",
    "display(tx_and_south_ss_codes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
