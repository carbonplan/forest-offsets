{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"50\" src=\"https://carbonplan-assets.s3.amazonaws.com/monogram/dark-small.png\" style=\"margin-left:0px;margin-top:20px\"/>\n",
    "\n",
    "# MTBS Fire Data\n",
    "\n",
    "_by Joe Hamman (CarbonPlan), December 17, 2020_\n",
    "\n",
    "We have produced a 4km monthly fire data product from the MTBS database.\n",
    "\n",
    "As background, MTBS itself provides two products: raw fire perimeter shapefiles\n",
    "with individual fire dates, and an annual-only \"burn severity\" mosaic that uses\n",
    "LANDAST data to ascribe pixel-level severity. We combined these to obtain a\n",
    "monthly raster product thresholded by burn severity.\n",
    "\n",
    "The steps to create this data product were as follows:\n",
    "\n",
    "- Extract fire perimeters directly from MTBS shape files for all fire categories\n",
    "  except for prescribed burns\n",
    "- Create a binary raster for each month by \"burning in\" shapes for all fires\n",
    "  with dates in that month\n",
    "- Mask each raster with the MTBS burn severity mosaic from the corresponding\n",
    "  year to include only moderate or high severity fires\n",
    "- Aggregate from 30m to 4km with averaging, treating masked regions in the\n",
    "  original data as missing values\n",
    "\n",
    "We independently validated that, in any given year, the sum of this monthly\n",
    "dataset across months was essentially identical to the annual burn severity\n",
    "mosoaic product restricted to the same severity thresholds. Any differences\n",
    "could be explained by the fact that the existing annual product appears to\n",
    "include prescribed burns, whereas we have excluded them.\n",
    "\n",
    "This notebook loads the 4km rasters and aggregates integrated risk to\n",
    "supersection and ecoregion. The resulting datasets are stored in GeoJSON for\n",
    "later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regionmask as rm\n",
    "import hvplot.pandas\n",
    "from retrospective.fire.project_risk import (\n",
    "    get_baileys,\n",
    "    get_supersections,\n",
    "    get_mtbs,\n",
    "    integrated_risk,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the monthly MTBS data and regional polygons (supersections and\n",
    "Bailey's ecoregions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"https://storage.googleapis.com/carbonplan-data\"\n",
    "\n",
    "da = get_mtbs(prefix, load=False).sel(time=slice(\"2001\", \"2018\")).load()\n",
    "baileys = get_baileys(prefix)\n",
    "supersections = get_supersections(prefix)\n",
    "regions = {\"baileys\": baileys, \"supersections\": supersections}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_series(da):\n",
    "    \"\"\"helper function to unpack risk data\"\"\"\n",
    "    s = da.to_series()\n",
    "    s.index = s.index.astype(int)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate fire risk and aggregate by region\n",
    "\n",
    "Now we will loop through the regions and create new GeoDataFrames that include a\n",
    "new column (`integrated_risk`). We do this by first rasterizing the regional\n",
    "polygons and calculating the aggregated risk for each polygon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_gdfs = {}\n",
    "\n",
    "for reg_name, region_gdf in regions.items():\n",
    "    print(len(region_gdf))\n",
    "    masks = rm.mask_geopandas(region_gdf, da)\n",
    "    avg_risk = da.groupby(masks).mean().groupby(\"time.year\").sum().mean(\"year\")\n",
    "    avg_risk_s = to_series(avg_risk)\n",
    "    region_gdf[\"integrated_risk\"] = np.nan\n",
    "    for i, val in avg_risk_s.iteritems():\n",
    "        region_gdf[\"integrated_risk\"][i] = integrated_risk(val)\n",
    "    out_gdfs[reg_name] = region_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fire risk data for all the polygons in each dataset, we can\n",
    "show it all on a map. First supersections, then Bailey's ecoregions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_gdfs[\"supersections\"].hvplot(\n",
    "    c=\"integrated_risk\",\n",
    "    cmap=\"YlOrRd\",\n",
    "    hover_cols=[\"SSection\", \"integrated_risk\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_gdfs[\"baileys\"].hvplot(\n",
    "    c=\"integrated_risk\", cmap=\"YlOrRd\", hover_cols=[\"integrated_risk\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write the expanded GeoDataFrame to GeoJSON files for later use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, gdf in out_gdfs.items():\n",
    "    dst_path = f\"/Users/jhamman/CarbonPlan Dropbox/Projects/Microsoft/Forests-Retrospective/carbonplan-retro/fire/{key}.json\"\n",
    "\n",
    "    with open(dst_path, \"w\") as f:\n",
    "        f.write(gdf.to_json(indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
