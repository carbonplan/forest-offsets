{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"50\" src=\"https://carbonplan-assets.s3.amazonaws.com/monogram/dark-small.png\" style=\"margin-left:0px;margin-top:20px\"/>\n",
    "\n",
    "# MTBS Fire Data\n",
    "\n",
    "_by Jeremy Freeman (CarbonPlan), November 28, 2020_\n",
    "\n",
    "We have produced a 4km monthly fire data product from the MTBS database.\n",
    "\n",
    "As background, MTBS itself provides two products: raw fire perimeter shapefiles\n",
    "with individual fire dates, and an annual-only \"burn severity\" mosaic that uses\n",
    "LANDAST data to ascribe pixel-level severity. We combined these to obtain a\n",
    "monthly raster product thresholded by burn severity.\n",
    "\n",
    "The steps to create this data product were as follows:\n",
    "\n",
    "- Extract fire perimeters directly from MTBS shape files for all fire categories\n",
    "  except for prescribed burns\n",
    "- Create a binary raster for each month by \"burning in\" shapes for all fires\n",
    "  with dates in that month\n",
    "- Mask each raster with the MTBS burn severity mosaic from the corresponding\n",
    "  year to include only moderate or high severity fires\n",
    "- Aggregate from 30m to 4km with averaging, treating masked regions in the\n",
    "  original data as missing values\n",
    "\n",
    "We independently validated that, in any given year, the sum of this monthly\n",
    "dataset across months was essentially identical to the annual burn severity\n",
    "mosoaic product restricted to the same severity thresholds. Any differences\n",
    "could be explained by the fact that the existing annual product appears to\n",
    "include prescribed burns, whereas we have excluded them.\n",
    "\n",
    "This notebook loads and inspects the data and demonstrates a few different kinds\n",
    "of queries against it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import geopandas as gp\n",
    "import regionmask as rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from palettable.colorbrewer.sequential import YlOrRd_9\n",
    "import matplotlib.pyplot as plt\n",
    "cmap = YlOrRd_9.mpl_colormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the monthly MTBS data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = fsspec.get_mapper(\n",
    "    \"gs://carbonplan-data/processed/mtbs/conus/4000m/monthly.zarr\"\n",
    ")\n",
    "da = xr.open_zarr(mapper)[\"monthly\"]\n",
    "da.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also load \"supersections\" which we'll use for computing regional\n",
    "statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = gp.read_file(\"data/supersections.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll compute masks from these regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = rm.mask_3D_geopandas(regions, da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll demo two examples of aggregating within regions to extract historical\n",
    "fire statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying individual regions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our masks to query individual regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.where(masks[0]).mean([\"x\", \"y\"]).plot(figsize=[15, 5])\n",
    "plt.title(regions.iloc[0].SSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.where(masks[1]).mean([\"x\", \"y\"]).plot(figsize=[15, 5])\n",
    "plt.title(regions.iloc[1].SSection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the same dataset to compute annual trends. Technically, there\n",
    "may be differences between the following two operations:\n",
    "\n",
    "- taking the sum over all months in a given year at 4km\n",
    "- taking the sum over all months in a given year at 30m, thresholding values\n",
    "  greater than 1, and downsampling to 4km\n",
    "\n",
    "Specifically, if there are repeat fires within the same 30m grid cell across\n",
    "months, the first method will slightly overestimate the annual burn area\n",
    "computed using the second method. Empirically, we compared the results of these\n",
    "two computations, and found effectively identical outputs, with the exception of\n",
    "one location in one year (1996) (not shown).\n",
    "\n",
    "This result implies that essentially no grid cells at 30m have repeat fires\n",
    "across months within a given year. It also implies that we can simply sum across\n",
    "months in the 4km product to get annual burn rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show annual trends for the same regions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.where(masks[0]).mean([\"x\", \"y\"]).groupby(\"time.year\").sum().plot(\n",
    "    figsize=[15, 5]\n",
    ")\n",
    "plt.title(regions.iloc[0].SSection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.where(masks[1]).mean([\"x\", \"y\"]).groupby(\"time.year\").sum().plot(\n",
    "    figsize=[15, 5]\n",
    ")\n",
    "plt.title(regions.iloc[0].SSection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire statistics: reproducing Anderegg et al. (2020)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we show we can reproduce the analysis from Figure 5 of Anderegg et al.\n",
    "(2020) with this new data product. That analysis used MTBS's annual rasters, but\n",
    "here we find the results are essentially identical (and now that we're using\n",
    "`xarray`, the code got a lot simpler!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define a function for converting annual risk to an \"100-year risk\", which\n",
    "estimates the probability of at least 1 fire in 100 years if all fires have a\n",
    "risk equal to the historical burn rate for that region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "\n",
    "\n",
    "def integrated_risk(p):\n",
    "    return (1 - binom.cdf(0, 100, p)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at the period from 1984 to 2000, first we sum within each year, and then\n",
    "average across years.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_2000 = (\n",
    "    da.sel(time=slice(\"1984\", \"2000\")).groupby(\"time.year\").sum().mean(\"year\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use our masks to compute the mean within each region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risks_before_2000 = np.asarray(\n",
    "    [\n",
    "        before_2000.where(masks.sel(region=i)).mean([\"x\", \"y\"]).values.item()\n",
    "        for i in masks[\"region\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions.to_crs(\"EPSG:5070\").plot(\n",
    "    integrated_risk(risks_before_2000),\n",
    "    figsize=[15, 8],\n",
    "    cmap=cmap,\n",
    "    edgecolor=[0, 0, 0],\n",
    "    linewidth=0.3,\n",
    "    vmin=0,\n",
    "    vmax=50,\n",
    "    legend=True,\n",
    ")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same thing for the period from 2001 to 2018\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_2000 = (\n",
    "    da.sel(time=slice(\"2001\", \"2017\")).groupby(\"time.year\").sum().mean(\"year\")\n",
    ")\n",
    "risks_after_2000 = np.asarray(\n",
    "    [\n",
    "        after_2000.where(masks.sel(region=i)).mean([\"x\", \"y\"]).values.item()\n",
    "        for i in masks[\"region\"]\n",
    "    ]\n",
    ")\n",
    "regions.to_crs(\"EPSG:5070\").plot(\n",
    "    integrated_risk(risks_after_2000),\n",
    "    figsize=[15, 8],\n",
    "    cmap=cmap,\n",
    "    edgecolor=[0, 0, 0],\n",
    "    linewidth=0.3,\n",
    "    vmin=0,\n",
    "    vmax=50,\n",
    "    legend=True,\n",
    ")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm these look essentially identical to the published Figure 5.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
