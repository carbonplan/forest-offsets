{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import geopandas as gpd\n",
    "import geoviews.feature as gf\n",
    "import cartopy\n",
    "import cartopy.feature as cf\n",
    "\n",
    "from geoviews import opts\n",
    "from cartopy import crs as ccrs\n",
    "\n",
    "gv.extension(\"matplotlib\", \"bokeh\")\n",
    "gv.output(dpi=120, fig=\"svg\")\n",
    "hv.output(backend=\"bokeh\")\n",
    "\n",
    "ss = gpd.read_file(\"/home/jovyan/carbonplan/shapes/Supersections/\")\n",
    "\n",
    "ss.geometry = ss.simplify(tolerance=5000)  # straightens out < 1km wiggles\n",
    "\n",
    "states = gpd.read_file(\n",
    "    \"https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_1_states_provinces.zip\"\n",
    ")\n",
    "ca = states.loc[states.name == \"California\"]\n",
    "ca_ecomap = gpd.overlay(ss.to_crs(ca.crs), ca, how=\"intersection\")\n",
    "ca_ecomap[\"highlight\"] = ca_ecomap[\"SSection\"] == \"Southern Cascades\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\"/home/jovyan/carbonplan/retrospective\")\n",
    "from retrospective.load.fia import load_fia_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previously]() we have extensively discussed how ARB calculates \"common practice.\" As a brief\n",
    "refresher, common practice is derived from the US Forest Service's forest plot network and is meant\n",
    "to describe the \"average carbon\" stored in all forested ecosystems, across the continential United\n",
    "States (and coastal Alaska).\n",
    "\n",
    "To define common practice, ARB first breaks CONUS into smaller \"supersections.\" Then, within those\n",
    "supersections, ARB defines a series of \"assessment areas\" or forest types. ARB then uses to\n",
    "calculate average \"standing live aboveground carbon\" from all FIA plots that fit those two screening\n",
    "criteria (geographic boundary, forest type). Most of the `retro` project ultimately boils down to\n",
    "evaluating the validity of how ARB (with the help of CAR) chose to define these geographic and\n",
    "forest type aggregations.\n",
    "\n",
    "This notebook helps motivate and introduce a series of analyses that will allow us to explore how\n",
    "the second criteria -- aggregating by species -- affects common practice.\n",
    "\n",
    "# The Problem\n",
    "\n",
    "Assessment areas are sometimes a concolomeration of species that really aren't all that similar. The\n",
    "`Southern Cascades` supersection, shown in green below, provides a pretty powerful example of this\n",
    "problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output backend='matplotlib', fig='svg'\n",
    "gv.Polygons(ca_ecomap, vdims=[\"highlight\"]).opts(cmap=\"Dark2_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll focus our discussion on the `Mixed Conifer` assessment area within the supersection. Notice\n",
    "how the supersection (the green thing above) spans from California's coast all the way to the\n",
    "northern Sierra foothills. Needless to say, this region includes an impressive array of forest\n",
    "types. The west includes lots and lots of big, productive Douglas fir forests. They look like this:\n",
    "![dougie](https://www.researchgate.net/profile/Aaron_Weiskittel/publication/283385356/figure/fig1/AS:291766009511936@1446573845818/Old-growth-stand-of-Douglas-fir-in.png)\n",
    "\n",
    "As you move to the east, things dry out and you get forests that look totally different. On the\n",
    "eastern edge of the supersection, you have lots of Ponderosa pine forests. They look like this:\n",
    "![pipo](https://assets.bwbx.io/images/users/iqjWHBFdfxIU/ivcyZMldwqn8/v0/360x-1.jpg)\n",
    "\n",
    "The visual differences are profound. More quantitatively, those differences translate into wildly\n",
    "different average carbon stocks across these two forest types. Using FIA data for California, we can\n",
    "precisely calculate this difference. Across all of California, Douglas fir plots have an average\n",
    "above-ground carbon stock of 129 t CO2e acre-1, while Ponderosa Pine have an average of 49 t CO2e\n",
    "acre-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrospective.load.fia import fia\n",
    "\n",
    "subset = pd.concat([fia(postal_code, kind=\"long\") for postal_code in [\"ca\", \"or\"]])\n",
    "\n",
    "subset[(subset[\"owner\"] == 46) & (subset[\"year\"] > 2001)].groupby(\"field_type\").slag_co2e_acre.agg(\n",
    "    [\"mean\", \"count\"]\n",
    ").loc[[201, 221]].rename({201: \"Douglas Fir\", 221: \"Ponderosa\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This long winded introduction gets us to the problem: for the `Southern Cascades` superseciton, ARB\n",
    "averages these dissimilar forests and reports a single common practice. That average, in turn, lies\n",
    "somewhere above what ponderosa pine forests _typically_ look like and somewhere below what doug fir\n",
    "forests _typically_ look like. This becomes important because projects are awarded credits for the\n",
    "amount of carbon within the project area is \"above common practice.\"\n",
    "\n",
    "We can be even more explicit and extend our simple Ponderosa/Dougie example one step further. Based\n",
    "on the above reasoning, we would expect to see projects in the `Southern Cascades Mixed Conifer`\n",
    "assessment area that have lots and lots of douglas fir. This would allow project developers to\n",
    "maximize the difference between their `initial carbon stock` and `common practice`. And this is\n",
    "exactly the behavior we see. If you open up the\n",
    "[project viewer](https://retro.staging.carbonplan.org/browser), you'll see that there is a strong\n",
    "clustering of projects along the western edge of the `Southern Cascades` supersection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Objective\n",
    "\n",
    "We want to evaluate what would happen if we comapred projects against similar looking forests and\n",
    "not the inaccurate of giant basket of potentially dissimilar forests.\n",
    "\n",
    "To accomplish this, we must first figure out an objective way to classify each project's forest\n",
    "type. While projects are required to report information about the species present in each project\n",
    "assessment area, they're under no obligation to report details on this semi-fuzzy concept of \"forest\n",
    "type.\"\n",
    "\n",
    "# Data\n",
    "\n",
    "## Offset project species composition\n",
    "\n",
    "For each project\\* we have information about species composition on a _per assessment area_ basis.\n",
    "Specifically, for each assessment area we have information about: species, total basal area, and\n",
    "basal area as a fraction of all basal area within that assessment area.\n",
    "\n",
    "These data are stored in a json object with the following schema:\n",
    "\n",
    "```json\n",
    "{<assessment_area_code>: [{\"code\": <fia_species_code>, \"basal_area\": float, \"fraction\": float} ...]\n",
    "```\n",
    "\n",
    "## FIA Trees\n",
    "\n",
    "The FIAdb contains information at the scale of individual trees.\n",
    "\n",
    "We're interested in the following attributes:\n",
    "\n",
    "- SPCD: a unique code that identifies the species of the tree\n",
    "- DIA: the tree's diameter at beast height\n",
    "- TPA_UNADJ: an \"adjustment factor\" that allows us to scale from a single tree to a \"per area\"\n",
    "  estimate of basal area\n",
    "- STATUSCD: Flag identifying if the tree is alive (1), dead (2), or removed (3). We're only\n",
    "  interested in live trees.\n",
    "\n",
    "For each stem, we calculate `unadjusted basal area` using the following equation:\n",
    "\n",
    "```equation\n",
    "unadjusted_basal_area = (DIA/2)**2 \\cdot \\Pi \\cdot TPA_UNADJ\n",
    "```\n",
    "\n",
    "## FIA Plots (Conditions)\n",
    "\n",
    "FIAdb contains details about \"plot condition\" -- we don't need to get into the details of how\n",
    "conditions differ from plots (it's pretty esoteric, but important when you're planning to measure\n",
    "100,000s of plots...). For our purposes, we only care about two details:\n",
    "\n",
    "- all trees from the TREE database maps to a single, unique condition.\n",
    "- each condition is assigned a forest type code\n",
    "\n",
    "In fact, conditions are assigned _two_ \"type codes\": a `FORTYPCD`, which is assigned by a computer\n",
    "algrorithm (woof -- we _do not_ trust this) and a `FLDTYPCD` (\"field type code\"), which is assigned\n",
    "by a trained forester based on the \"balance of evidence\" they observe while physically visiting the\n",
    "plot.\n",
    "\n",
    "# Model Structure\n",
    "\n",
    "We are going to train some flavor of a classification model. It might even be an _ensemble_\n",
    "classifier (!). The exact algorithm isn't really that important. All models, however, will share the\n",
    "same training data -- features and targets -- which I will describe here.\n",
    "\n",
    "## Targets\n",
    "\n",
    "The target variable is forest type, either `FORTYPCD` or `FLDTYPCD`.\n",
    "\n",
    "## Features\n",
    "\n",
    "The feature set will consist of per-species estimates of `fractional basal area`, aggregated to the\n",
    "condition level.\n",
    "\n",
    "\\*: species information remains incomplete for many projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where are we already?\n",
    "\n",
    "Okay so we can load data! I made a little function (`load_fia_tree` that lives inside\n",
    "`retrospective/load/fia.py`) which handles the data loading. For this tutorial, we'll just load a\n",
    "single state. However, for training the model, it's imperative that we train a single model for\n",
    "_all_ data. This is because we do not want to make artificial geographic cutoffs about where certain\n",
    "`FORTYPCDS` occur. Sure, there are zero doug fir forests on the East Coast, but I'm pretty sure that\n",
    "sitting down to make an exhasutive, biologically grounded list of which regions to include, on a per\n",
    "FORTYPCD basis, would (i) take forever and (ii) be wrong. Let's skip the hassle and just chuck it\n",
    "all in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# its a demo, don't judge\n",
    "try:\n",
    "    tree_df = pd.read_csv(\"/home/jovyan/lost+found/ca_tree.csv\")\n",
    "except:\n",
    "    tree_df = load_fia_tree(\"ca\")\n",
    "    tree_df.to_csv(\"/home/jovyan/lost+found/ca_tree.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we also have a really simple function that calculates fractional basal area on a species basis.\n",
    "This is directly comporable to the information we have about species from the project db.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractional_basal_area_by_species(data):\n",
    "    \"\"\"For group of trees, calcuate the fraction total basal area represented by each species\"\"\"\n",
    "    # cast to str so can store sparsely :)\n",
    "    weights = (\n",
    "        data.groupby(data[\"SPCD\"].astype(str)).unadj_basal_area.sum() / data.unadj_basal_area.sum()\n",
    "    ).round(4)\n",
    "    weights = weights.to_dict()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then generate our feature set and our target variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tree_df.groupby([\"PLT_CN\", \"CONDID\"]).apply(fractional_basal_area_by_species)\n",
    "targets = tree_df.groupby([\"PLT_CN\", \"CONDID\"])[[\"FORTYPCD\", \"FLDTYPCD\"]].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features is now a big series where the index is a unique plot-condition and the series contains a\n",
    "bunch of dicts with the following schema:\n",
    "\n",
    "```json\n",
    "{<species_code>: <fractional_basal_area> ...}\n",
    "\n",
    "```\n",
    "\n",
    "They look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for _, x in features.sample(10).items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick note on FLDTYPCDs\n",
    "\n",
    "FLDTYP can be null. I don't know why this is the case -- but it happens. Gotta remove/handle those\n",
    "before we train? That said, I'm pretty sure we trust `FLDTYPCD` more than `FORTYPCD` so...who knows.\n",
    "\n",
    "Oh! There is also a special `FORTYPCD`: 999. 999 represents an \"unstocked\" condition -- meaning\n",
    "there are just no trees there anymore, either due to harvest or some sort of mortality event (e.g.,\n",
    "buggies). I remove those data below. But I'll just write this down here -- ARB actually included\n",
    "these unstocked plots in their estimates of common practice! Whoops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i doubt this join is strictly necessary, given that im 99 percent sure features and targets have the same ordering -- but yeah...im paranoid.\n",
    "full = targets.join(features.rename(\"feature_lst\"))\n",
    "full = full.loc[(full[\"FLDTYPCD\"] != 999)]\n",
    "full = full.dropna(subset=[\"FLDTYPCD\"])  # cant have nan in target. careful here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use DictVectorizer to make a nice `sklearn` ready feature array. I'll explode the array\n",
    "to its dense format with `toarray()` -- but certain algrorithms can actually train on the sparse\n",
    "represetation. I realize we can just get a big enough machine -- but as someone who once hand rolled\n",
    "a sparse matrix representation (i was young and foolish) -- i find the fact that you can get\n",
    "spareness this easily just super fascinating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "X = vec.fit_transform(\n",
    "    full[\"feature_lst\"].values\n",
    ").toarray()  # .toarray() explodes the sparse array returned from DictVectorizer() out into a dense array\n",
    "y = full[\"FLDTYPCD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split our dataset into train and testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test subset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some models and train them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict of classifiers we can just train doing nothing fancy at all!\n",
    "clfs = {\n",
    "    \"naive_ bayes\": MultinomialNB(),\n",
    "    \"linear_svm\": SVC(kernel=\"linear\", C=0.025),\n",
    "    \"random_forest\": RandomForestClassifier(),\n",
    "}\n",
    "\n",
    "for name, clf in clfs.items():\n",
    "    y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(f\"{name} mislabled {(y_test != y_pred).sum()} of {X_test.shape[0]} points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What i think is the right next step\n",
    "\n",
    "As you can see from above -- this thing is fairly decent out of the box. While we'd love to have\n",
    "good accuracy, we're actually fine if the model returns a range of possible classifications -- we\n",
    "can take the uncertainty in the classification and propogate that through to our final estiamte of\n",
    "common practice using brute force sampling. That said, I think that some simple hyperparamertization\n",
    "will go a long way. Given that RF is arleady so good, maybe the next step is to just fiddle around\n",
    "with RF parameters (e.g., grow more trees, change splitting criteria, etc):\n",
    "\n",
    "```python\n",
    "rf_clf = RandomForestClassifier(n_estimators=1500)\n",
    "mod = rf_clf.fit(X_train, y_train)\n",
    "y_preds = mod.predict(X_test)\n",
    "```\n",
    "\n",
    "My suspicion is that after some parameter tuning, we should just grow a massive RF classifier using\n",
    "dask (that should be fun!) and call it a day.\n",
    "\n",
    "We also probably want to spend some time thinking about FLDTYPS we dont really care about -- I doubt\n",
    "the answer is to exclude data. At the same time, some FLDTYPCDs are going to be super rare -- rare\n",
    "to the point where building a training/validation dataset is going just be plain tough. Obviously\n",
    "model performance will be driven by the more common TYPS, but I don't want us spending cycles\n",
    "chasing performance on TYPS that don't matter...\n",
    "\n",
    "OH! And it's _really_ important that we\n",
    "[calibrate the classification probabilities](https://scikit-learn.org/stable/modules/calibration.html)\n",
    "from our model. If we're going to allow fuzzy matching (which we should!), we need to make sure that\n",
    "the model reported probabilites are \"real\" and do not contain artifacts that arise from the\n",
    "underlying classificaiton model (e.g., how RF deal with bagging influences the values produced by\n",
    "`.predict_proba()`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other sklearn magic I discovered...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(classification_report(y_test, y_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test, y_pred)\n",
    "plt.scatter(recall, support)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
