{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "import cartopy\n",
    "import cartopy.feature as cf\n",
    "\n",
    "from geoviews import opts\n",
    "from cartopy import crs as ccrs\n",
    "\n",
    "gv.extension(\"matplotlib\", \"bokeh\")\n",
    "\n",
    "gv.output(dpi=120, fig=\"svg\")\n",
    "\n",
    "hv.output(backend=\"bokeh\")\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "ss = gpd.read_file(\"/home/jovyan/carbonplan/shapes/Supersections/\")\n",
    "\n",
    "ss.geometry = ss.simplify(tolerance=5000)  # straightens out < 1km wiggles\n",
    "\n",
    "states = gpd.read_file(\n",
    "    \"https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/110m/cultural/ne_110m_admin_1_states_provinces.zip\"\n",
    ")\n",
    "ca = states.loc[states.name == \"California\"]\n",
    "ca_ecomap = gpd.overlay(ss.to_crs(ca.crs), ca, how=\"intersection\")\n",
    "ca_ecomap[\"highlight\"] = ca_ecomap[\"SSection\"] == \"Southern Cascades\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\"/home/jovyan/carbonplan/retrospective\")\n",
    "from retrospective.load.fia import load_fia_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Previously]() we have extensively discussed how ARB calculates \"common\n",
    "practice.\" As a brief refresher, common practice is derived from the US Forest\n",
    "Service's forest plot network and is meant to describe the \"average carbon\"\n",
    "stored in all forested ecosystems, across the continential United States (and\n",
    "coastal Alaska).\n",
    "\n",
    "To define common practice, ARB first breaks CONUS into smaller \"supersections.\"\n",
    "Then, within those supersections, ARB defines a series of \"assessment areas\" or\n",
    "forest types. ARB then uses to calculate average \"standing live aboveground\n",
    "carbon\" from all FIA plots that fit those two screening criteria (geographic\n",
    "boundary, forest type). Most of the `retro` project ultimately boils down to\n",
    "evaluating the validity of how ARB (with the help of CAR) chose to define these\n",
    "geographic and forest type aggregations.\n",
    "\n",
    "This notebook helps motivate and introduce a series of analyses that will allow\n",
    "us to explore how the second criteria -- aggregating by species -- affects\n",
    "common practice.\n",
    "\n",
    "# The Problem\n",
    "\n",
    "Assessment areas are sometimes a concolomeration of species that really aren't\n",
    "all that similar. The `Southern Cascades` supersection, shown in green below,\n",
    "provides a pretty powerful example of this problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%output backend='matplotlib', fig='svg'\n",
    "gv.Polygons(ca_ecomap, vdims=[\"highlight\"]).opts(cmap=\"Dark2_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll focus our discussion on the `Mixed Conifer` assessment area within the\n",
    "supersection. Notice how the supersection (the green thing above) spans from\n",
    "California's coast all the way to the northern Sierra foothills. Needless to\n",
    "say, this region includes an impressive array of forest types. The west includes\n",
    "lots and lots of big, productive Douglas fir forests. They look like this:\n",
    "![dougie](https://www.researchgate.net/profile/Aaron_Weiskittel/publication/283385356/figure/fig1/AS:291766009511936@1446573845818/Old-growth-stand-of-Douglas-fir-in.png)\n",
    "\n",
    "As you move to the east, things dry out and you get forests that look totally\n",
    "different. On the eastern edge of the supersection, you have lots of Ponderosa\n",
    "pine forests. They look like this:\n",
    "![pipo](https://assets.bwbx.io/images/users/iqjWHBFdfxIU/ivcyZMldwqn8/v0/360x-1.jpg)\n",
    "\n",
    "The visual differences are profound. More quantitatively, those differences\n",
    "translate into wildly different average carbon stocks across these two forest\n",
    "types. Using FIA data for California, we can precisely calculate this\n",
    "difference. Across all of California, Douglas fir plots have an average\n",
    "above-ground carbon stock of 129 t CO2e acre-1, while Ponderosa Pine have an\n",
    "average of 49 t CO2e acre-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retrospective.load.fia import fia\n",
    "\n",
    "subset = pd.concat(\n",
    "    [fia(postal_code, kind=\"long\") for postal_code in [\"ca\", \"or\"]]\n",
    ")\n",
    "\n",
    "subset[(subset[\"owner\"] == 46) & (subset[\"year\"] > 2001)].groupby(\n",
    "    \"field_type\"\n",
    ").slag_co2e_acre.agg([\"mean\", \"count\"]).loc[[201, 221]].rename(\n",
    "    {201: \"Douglas Fir\", 221: \"Ponderosa\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This long winded introduction gets us to the problem: for the\n",
    "`Southern Cascades` superseciton, ARB averages these dissimilar forests and\n",
    "reports a single common practice. That average, in turn, lies somewhere above\n",
    "what ponderosa pine forests _typically_ look like and somewhere below what doug\n",
    "fir forests _typically_ look like. This becomes important because projects are\n",
    "awarded credits for the amount of carbon within the project area is \"above\n",
    "common practice.\"\n",
    "\n",
    "We can be even more explicit and extend our simple Ponderosa/Dougie example one\n",
    "step further. Based on the above reasoning, we would expect to see projects in\n",
    "the `Southern Cascades Mixed Conifer` assessment area that have lots and lots of\n",
    "douglas fir. This would allow project developers to maximize the difference\n",
    "between their `initial carbon stock` and `common practice`. And this is exactly\n",
    "the behavior we see. If you open up the\n",
    "[project viewer](https://retro.staging.carbonplan.org/browser), you'll see that\n",
    "there is a strong clustering of projects along the western edge of the\n",
    "`Southern Cascades` supersection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Objective\n",
    "\n",
    "We want to evaluate what would happen if we comapred projects against similar\n",
    "looking forests and not the inaccurate of giant basket of potentially dissimilar\n",
    "forests.\n",
    "\n",
    "To accomplish this, we must first figure out an objective way to classify each\n",
    "project's forest type. While projects are required to report information about\n",
    "the species present in each project assessment area, they're under no obligation\n",
    "to report details on this semi-fuzzy concept of \"forest type.\"\n",
    "\n",
    "# Data\n",
    "\n",
    "## Offset project species composition\n",
    "\n",
    "For each project\\* we have information about species composition on a _per\n",
    "assessment area_ basis. Specifically, for each assessment area we have\n",
    "information about: species, total basal area, and basal area as a fraction of\n",
    "all basal area within that assessment area.\n",
    "\n",
    "These data are stored in a json object with the following schema:\n",
    "\n",
    "```json\n",
    "{<assessment_area_code>: [{\"code\": <fia_species_code>, \"basal_area\": float, \"fraction\": float} ...]\n",
    "```\n",
    "\n",
    "## FIA Trees\n",
    "\n",
    "The FIAdb contains information at the scale of individual trees.\n",
    "\n",
    "We're interested in the following attributes:\n",
    "\n",
    "- SPCD: a unique code that identifies the species of the tree\n",
    "- DIA: the tree's diameter at beast height\n",
    "- TPA_UNADJ: an \"adjustment factor\" that allows us to scale from a single tree\n",
    "  to a \"per area\" estimate of basal area\n",
    "- STATUSCD: Flag identifying if the tree is alive (1), dead (2), or removed (3).\n",
    "  We're only interested in live trees.\n",
    "\n",
    "For each stem, we calculate `unadjusted basal area` using the following\n",
    "equation:\n",
    "\n",
    "```equation\n",
    "unadjusted_basal_area = (DIA/2)**2 \\cdot \\Pi \\cdot TPA_UNADJ\n",
    "```\n",
    "\n",
    "## FIA Plots (Conditions)\n",
    "\n",
    "FIAdb contains details about \"plot condition\" -- we don't need to get into the\n",
    "details of how conditions differ from plots (it's pretty esoteric, but important\n",
    "when you're planning to measure 100,000s of plots...). For our purposes, we only\n",
    "care about two details:\n",
    "\n",
    "- all trees from the TREE database maps to a single, unique condition.\n",
    "- each condition is assigned a forest type code\n",
    "\n",
    "In fact, conditions are assigned _two_ \"type codes\": a `FORTYPCD`, which is\n",
    "assigned by a computer algrorithm (woof -- we _do not_ trust this) and a\n",
    "`FLDTYPCD` (\"field type code\"), which is assigned by a trained forester based on\n",
    "the \"balance of evidence\" they observe while physically visiting the plot.\n",
    "\n",
    "# Model Structure\n",
    "\n",
    "We are going to train some flavor of a classification model. It might even be an\n",
    "_ensemble_ classifier (!). The exact algorithm isn't really that important. All\n",
    "models, however, will share the same training data -- features and targets --\n",
    "which I will describe here.\n",
    "\n",
    "## Targets\n",
    "\n",
    "The target variable is forest type, either `FORTYPCD` or `FLDTYPCD`.\n",
    "\n",
    "## Features\n",
    "\n",
    "The feature set will consist of per-species estimates of\n",
    "`fractional basal area`, aggregated to the condition level.\n",
    "\n",
    "\\*: species information remains incomplete for many projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where are we already?\n",
    "\n",
    "Okay so we can load data! I made a little function (`load_fia_tree` that lives\n",
    "inside `retrospective/load/fia.py`) which handles the data loading. For this\n",
    "tutorial, we'll just load a single state. However, for training the model, it's\n",
    "imperative that we train a single model for _all_ data. This is because we do\n",
    "not want to make artificial geographic cutoffs about where certain `FORTYPCDS`\n",
    "occur. Sure, there are zero doug fir forests on the East Coast, but I'm pretty\n",
    "sure that sitting down to make an exhasutive, biologically grounded list of\n",
    "which regions to include, on a per FORTYPCD basis, would (i) take forever and\n",
    "(ii) be wrong. Let's skip the hassle and just chuck it all in memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# its a demo, don't judge\n",
    "try:\n",
    "    tree_df = pd.read_csv(\"/home/jovyan/lost+found/ca_tree.csv\")\n",
    "except:\n",
    "    tree_df = load_fia_tree(\"ca\")\n",
    "    tree_df.to_csv(\"/home/jovyan/lost+found/ca_tree.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we also have a really simple function that calculates fractional basal area\n",
    "on a species basis. This is directly comporable to the information we have about\n",
    "species from the project db.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fractional_basal_area_by_species(data):\n",
    "    \"\"\"For group of trees, calcuate the fraction total basal area represented by each species\"\"\"\n",
    "    # cast to str so can store sparsely :)\n",
    "    weights = (\n",
    "        data.groupby(data[\"SPCD\"].astype(str)).unadj_basal_area.sum()\n",
    "        / data.unadj_basal_area.sum()\n",
    "    ).round(4)\n",
    "    weights = weights.to_dict()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then generate our feature set and our target variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tree_df.groupby([\"PLT_CN\", \"CONDID\"]).apply(\n",
    "    fractional_basal_area_by_species\n",
    ")\n",
    "targets = tree_df.groupby([\"PLT_CN\", \"CONDID\"])[[\"FORTYPCD\", \"FLDTYPCD\"]].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features is now a big series where the index is a unique plot-condition and the\n",
    "series contains a bunch of dicts with the following schema:\n",
    "\n",
    "```json\n",
    "{<species_code>: <fractional_basal_area> ...}\n",
    "\n",
    "```\n",
    "\n",
    "They look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for _, x in features.sample(10).items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quick note on FLDTYPCDs\n",
    "\n",
    "FLDTYP can be null. I don't know why this is the case -- but it happens. Gotta\n",
    "remove/handle those before we train? That said, I'm pretty sure we trust\n",
    "`FLDTYPCD` more than `FORTYPCD` so...who knows.\n",
    "\n",
    "Oh! There is also a special `FORTYPCD`: 999. 999 represents an \"unstocked\"\n",
    "condition -- meaning there are just no trees there anymore, either due to\n",
    "harvest or some sort of mortality event (e.g., buggies). I remove those data\n",
    "below. But I'll just write this down here -- ARB actually included these\n",
    "unstocked plots in their estimates of common practice! Whoops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i doubt this join is strictly necessary, given that im 99 percent sure features and targets have the same ordering -- but yeah...im paranoid.\n",
    "full = targets.join(features.rename(\"feature_lst\"))\n",
    "full = full.loc[(full[\"FLDTYPCD\"] != 999)]\n",
    "full = full.dropna(\n",
    "    subset=[\"FLDTYPCD\"]\n",
    ")  # cant have nan in target. careful here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use DictVectorizer to make a nice `sklearn` ready feature array.\n",
    "I'll explode the array to its dense format with `toarray()` -- but certain\n",
    "algrorithms can actually train on the sparse represetation. I realize we can\n",
    "just get a big enough machine -- but as someone who once hand rolled a sparse\n",
    "matrix representation (i was young and foolish) -- i find the fact that you can\n",
    "get spareness this easily just super fascinating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "X = vec.fit_transform(\n",
    "    full[\"feature_lst\"].values\n",
    ").toarray()  # .toarray() explodes the sparse array returned from DictVectorizer() out into a dense array\n",
    "y = full[\"FLDTYPCD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split our dataset into train and testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test subset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some models and train them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict of classifiers we can just train doing nothing fancy at all!\n",
    "clfs = {\n",
    "    \"naive_ bayes\": MultinomialNB(),\n",
    "    \"linear_svm\": SVC(kernel=\"linear\", C=0.025),\n",
    "    \"random_forest\": RandomForestClassifier(),\n",
    "}\n",
    "\n",
    "for name, clf in clfs.items():\n",
    "    y_pred = clf.fit(X_train, y_train).predict(X_test)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    print(\n",
    "        f\"{name} mislabled {(y_test != y_pred).sum()} of {X_test.shape[0]} points.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What i think is the right next step\n",
    "\n",
    "As you can see from above -- this thing is fairly decent out of the box. While\n",
    "we'd love to have good accuracy, we're actually fine if the model returns a\n",
    "range of possible classifications -- we can take the uncertainty in the\n",
    "classification and propogate that through to our final estiamte of common\n",
    "practice using brute force sampling. That said, I think that some simple\n",
    "hyperparamertization will go a long way. Given that RF is arleady so good, maybe\n",
    "the next step is to just fiddle around with RF parameters (e.g., grow more\n",
    "trees, change splitting criteria, etc):\n",
    "\n",
    "```python\n",
    "rf_clf = RandomForestClassifier(n_estimators=1500)\n",
    "mod = rf_clf.fit(X_train, y_train)\n",
    "y_preds = mod.predict(X_test)\n",
    "```\n",
    "\n",
    "My suspicion is that after some parameter tuning, we should just grow a massive\n",
    "RF classifier using dask (that should be fun!) and call it a day.\n",
    "\n",
    "We also probably want to spend some time thinking about FLDTYPS we dont really\n",
    "care about -- I doubt the answer is to exclude data. At the same time, some\n",
    "FLDTYPCDs are going to be super rare -- rare to the point where building a\n",
    "training/validation dataset is going just be plain tough. Obviously model\n",
    "performance will be driven by the more common TYPS, but I don't want us spending\n",
    "cycles chasing performance on TYPS that don't matter...\n",
    "\n",
    "OH! And it's _really_ important that we\n",
    "[calibrate the classification probabilities](https://scikit-learn.org/stable/modules/calibration.html)\n",
    "from our model. If we're going to allow fuzzy matching (which we should!), we\n",
    "need to make sure that the model reported probabilites are \"real\" and do not\n",
    "contain artifacts that arise from the underlying classificaiton model (e.g., how\n",
    "RF deal with bagging influences the values produced by `.predict_proba()`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other sklearn magic I discovered...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(classification_report(y_test, y_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "precision, recall, fscore, support = precision_recall_fscore_support(y_test,\n",
    "y_pred) plt.scatter(recall, support)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
